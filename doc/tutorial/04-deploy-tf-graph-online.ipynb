{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型在线使用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在03-split-to-sub-graph.ipynb中保存的模型是`TensorFlow`标准的[SavedModel](https://www.tensorflow.org/guide/saved_model)格式，下面将在前文demo的基础上，继续介绍如何将离线的模型部署到线上提供服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 编译模型\n",
    "- 使用`saved_model_cli aot_compile_cpu`编译模型\n",
    "  \n",
    "  编译的模型是SavedModel格式的文件夹，如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/wide-deep-test/model/tmp\r\n",
      "├── assets\r\n",
      "├── saved_model.pb\r\n",
      "└── variables\r\n",
      "    ├── variables.data-00000-of-00001\r\n",
      "    └── variables.index\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/wide-deep-test/model/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-26 14:27:15.353717: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2020-08-26 14:27:15.354013: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2020-08-26 14:27:15.366369: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2593325000 Hz\n",
      "2020-08-26 14:27:15.368276: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f7194f85a90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-26 14:27:15.368314: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-08-26 14:27:15.480529: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:563] model_pruner failed: Invalid argument: Invalid input graph.\n",
      "2020-08-26 14:27:15.485756: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\n",
      "2020-08-26 14:27:15.485803: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   model_pruner: Graph size after: 32 nodes (0), 46 edges (0), time = 1.903ms.\n",
      "2020-08-26 14:27:15.485825: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   implementation_selector: Graph size after: 32 nodes (0), 46 edges (0), time = 0.356ms.\n",
      "2020-08-26 14:27:15.485838: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 169 nodes (137), 304 edges (258), time = 25.105ms.\n",
      "2020-08-26 14:27:15.485849: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 83 nodes (-86), 131 edges (-173), time = 37.184ms.\n",
      "2020-08-26 14:27:15.485859: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   shape_optimizer: shape_optimizer did nothing. time = 0.236ms.\n",
      "2020-08-26 14:27:15.485869: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   arithmetic_optimizer: Graph size after: 82 nodes (-1), 132 edges (1), time = 2.817ms.\n",
      "2020-08-26 14:27:15.485879: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   layout: layout did nothing. time = 0.94ms.\n",
      "2020-08-26 14:27:15.485889: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   remapper: Graph size after: 82 nodes (0), 132 edges (0), time = 1.952ms.\n",
      "2020-08-26 14:27:15.485899: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   loop_optimizer: Graph size after: 82 nodes (0), 132 edges (0), time = 0.907ms.\n",
      "2020-08-26 14:27:15.485909: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   dependency_optimizer: Graph size after: 51 nodes (-31), 65 edges (-67), time = 1.885ms.\n",
      "2020-08-26 14:27:15.485919: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   memory_optimizer: Graph size after: 51 nodes (0), 65 edges (0), time = 4.456ms.\n",
      "2020-08-26 14:27:15.485929: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   model_pruner: Invalid argument: Invalid input graph.\n",
      "2020-08-26 14:27:15.485939: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   implementation_selector: Graph size after: 51 nodes (0), 65 edges (0), time = 0.098ms.\n",
      "2020-08-26 14:27:15.485949: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.049ms.\n",
      "2020-08-26 14:27:15.485959: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 51 nodes (0), 65 edges (0), time = 1.544ms.\n",
      "2020-08-26 14:27:15.485968: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   shape_optimizer: shape_optimizer did nothing. time = 0.06ms.\n",
      "2020-08-26 14:27:15.485979: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   arithmetic_optimizer: Graph size after: 51 nodes (0), 65 edges (0), time = 1.402ms.\n",
      "2020-08-26 14:27:15.485988: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   remapper: Graph size after: 51 nodes (0), 65 edges (0), time = 0.484ms.\n",
      "2020-08-26 14:27:15.485998: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   dependency_optimizer: Graph size after: 51 nodes (0), 65 edges (0), time = 0.703ms.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/wide-deep-test/model/tmp/variables/variables\n",
      "WARNING:tensorflow:From /da1/s/yaolei/anaconda3/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_aot_compile.py:332: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /da1/s/yaolei/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:359: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "INFO:tensorflow:Writing graph def to: /tmp/saved_model_clitv_0xugi/frozen_graph.pb\n",
      "INFO:tensorflow:Writing config_pbtxt to: /tmp/saved_model_clitv_0xugi/config.pbtxt\n",
      "INFO:tensorflow:Generating XLA AOT artifacts in: /tmp/model/online_serving/graph\n"
     ]
    }
   ],
   "source": [
    "!/da1/s/yaolei/anaconda3/bin/saved_model_cli aot_compile_cpu \\\n",
    "                                 --dir /tmp/wide-deep-test/model/tmp  \\\n",
    "                                 --tag_set serve \\\n",
    "                                 --output_prefix /tmp/model/online_serving/graph/graph \\\n",
    "                                 --cpp_class Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/model/online_serving/graph/\r\n",
      "├── graph.h\r\n",
      "├── graph_makefile.inc\r\n",
      "├── graph_metadata.o\r\n",
      "└── graph.o\r\n",
      "\r\n",
      "0 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/model/online_serving/graph/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tips**:\n",
    "\n",
    "tensorflow 安装包需要在安装时添加`--define=with_xla_support=true`选项，否则编译模型时会报错，错误如下：\n",
    "\n",
    "```bash\n",
    "Traceback (most recent call last):\n",
    "  File \"/da1/s/yaolei/anaconda3/bin/saved_model_cli\", line 8, in <module>\n",
    "    sys.exit(main())\n",
    "  File \"/da1/s/yaolei/anaconda3/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 1153, in main\n",
    "    args.func(args)\n",
    "  File \"/da1/s/yaolei/anaconda3/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 811, in aot_compile_cpu\n",
    "    enable_multithreading=args.enable_multithreading)\n",
    "  File \"/da1/s/yaolei/anaconda3/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_aot_compile.py\", line 258, in aot_compile_cpu_meta_graph_def\n",
    "    raise _pywrap_tfcompile_import_error\n",
    "ImportError: Unable to import _pywrap_tfcompile; you must build TensorFlow with XLA.  You may need to build tensorflow with flag --define=with_xla_support=true.  Original error: cannot import name '_pywrap_tfcompile' from 'tensorflow.python' (/da1/s/yaolei/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 调用模型\n",
    "- 编写graph.cc调用模型\n",
    "\n",
    "  examples下面提供了脚本一键生成graph.cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ grep 'static constexpr size_t kNumArgs' /da1/s/yaolei/tensornet/examples/online_serving/graph.h\n",
      "++ awk '-F;' '{print $1}'\n",
      "++ awk '{print $NF}'\n",
      "+ slot_num=8\n",
      "+ python3 gen_graph.py 8\n",
      "+ '[' 0 -ne 0 ']'\n"
     ]
    }
   ],
   "source": [
    " !cd /da1/s/yaolei/tensornet/examples/online_serving/ && sh graph_cc_generator.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// This file is MACHINE GENERATED! Do not edit.\r\n",
      "// Source file is gen_graph.py\r\n",
      "\r\n",
      "#include \"examples/online_serving/graph.h\"\r\n",
      "\r\n",
      "#include <vector>\r\n",
      "\r\n",
      "#define EIGEN_USE_THREADS\r\n",
      "#define EIGEN_USE_CUSTOM_THREAD_POOL\r\n",
      "\r\n",
      "#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n",
      "\r\n",
      "extern \"C\" int Run(const std::vector<std::vector<std::vector<float>>>& input,\r\n",
      "                   std::vector<float>& output) {\r\n",
      "    Eigen::ThreadPool tp(std::thread::hardware_concurrency());\r\n",
      "    Eigen::ThreadPoolDevice device(&tp, tp.NumThreads());\r\n",
      "    Graph graph;\r\n",
      "    graph.set_thread_pool(&device);\r\n",
      "\r\n",
      "    std::vector<int> dim = {1, 8};\r\n",
      "    for (size_t i = 0; i < input.size(); ++i) {\r\n",
      "        if (input[i].size() != Graph::kNumArgs / 2) {\r\n",
      "            std::cerr << \"TFFeaValues size is wrong, expected \" << Graph::kNumArgs / 2 << \" but get\" << input[i].size() << std::endl;\r\n",
      "            return -1;\r\n",
      "        }\r\n",
      "        if (input[i][0].size() != dim[0] + dim[1]) {\r\n",
      "            std::cerr << \"embedding size is wrong, expected \" << dim[0] + dim[1] << \" but get\" << input[i][0].size() << std::endl;\r\n",
      "            return -1;\r\n",
      "        }\r\n",
      "    \r\n",
      "\r\n",
      "        std::copy(input[i][0].data(), input[i][0].data() + dim[0], graph.arg_feed_wide_emb_0_data() + i * dim[0]);\r\n",
      "        std::copy(input[i][0].data() + dim[0], input[i][0].data() + dim[0] + dim[1], graph.arg_feed_deep_emb_0_data() + i * dim[1]);\r\n",
      "\r\n",
      "        std::copy(input[i][1].data(), input[i][1].data() + dim[0], graph.arg_feed_wide_emb_1_data() + i * dim[0]);\r\n",
      "        std::copy(input[i][1].data() + dim[0], input[i][1].data() + dim[0] + dim[1], graph.arg_feed_deep_emb_1_data() + i * dim[1]);\r\n",
      "\r\n",
      "        std::copy(input[i][2].data(), input[i][2].data() + dim[0], graph.arg_feed_wide_emb_2_data() + i * dim[0]);\r\n",
      "        std::copy(input[i][2].data() + dim[0], input[i][2].data() + dim[0] + dim[1], graph.arg_feed_deep_emb_2_data() + i * dim[1]);\r\n",
      "\r\n",
      "        std::copy(input[i][3].data(), input[i][3].data() + dim[0], graph.arg_feed_wide_emb_3_data() + i * dim[0]);\r\n",
      "        std::copy(input[i][3].data() + dim[0], input[i][3].data() + dim[0] + dim[1], graph.arg_feed_deep_emb_3_data() + i * dim[1]);\r\n",
      "\r\n",
      "    }\r\n",
      "\r\n",
      "    auto ok = graph.Run();\r\n",
      "    if (!ok) {\r\n",
      "        return -1;\r\n",
      "    }\r\n",
      "\r\n",
      "    output.clear();\r\n",
      "    output.assign(input.size(), 0.0);\r\n",
      "    std::copy(graph.result0_data(), graph.result0_data() + input.size(), output.data());\r\n",
      "\r\n",
      "    return 0;\r\n",
      "}\r\n",
      "\r\n",
      "    \r\n"
     ]
    }
   ],
   "source": [
    "!cat /da1/s/yaolei/tensornet/examples/online_serving/graph.cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 编译最终使用的动态库\n",
    "\n",
    "- 编写bazel编译代码，**需要注意的是**，这里将aot编译的graph.o重命名为graph_c.o，避免在编译graph.cc时发生冲突。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filegroup(\r\n",
      "    name = \"graph\",\r\n",
      "    srcs = [\r\n",
      "        \"online_serving/graph.cc\",\r\n",
      "        \"online_serving/graph_c.o\",\r\n",
      "        \"online_serving/graph.h\",\r\n",
      "    ],\r\n",
      ")\r\n",
      "\r\n",
      "cc_binary(\r\n",
      "    name = \"libmodel.so\",\r\n",
      "    srcs = [\":graph\"],\r\n",
      "    deps = [\r\n",
      "        \"@org_tensorflow//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\r\n",
      "        \"@org_tensorflow//tensorflow/core:framework_lite\",\r\n",
      "        \"@org_tensorflow//tensorflow/compiler/xla/service/cpu:runtime_conv2d\",\r\n",
      "        \"@org_tensorflow//tensorflow/compiler/xla/service/cpu:runtime_key_value_sort\",\r\n",
      "        \"@org_tensorflow//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\r\n",
      "        \"@org_tensorflow//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\",\r\n",
      "        \"@org_tensorflow//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\",\r\n",
      "        \"@org_tensorflow//third_party/eigen3:eigen3\",\r\n",
      "    ],\r\n",
      "    linkshared = 1,\r\n",
      "    linkopts = [\"-lpthread\"],\r\n",
      "    copts = [\"-fPIC\"],\r\n",
      ")\r\n",
      "\r\n",
      "cc_binary(\r\n",
      "    name = \"tf_serving\",\r\n",
      "    srcs = [\r\n",
      "        \"online_serving/main.cc\",\r\n",
      "        \"online_serving/random.h\",\r\n",
      "    ],\r\n",
      "    deps = [\r\n",
      "        \"@boost//:algorithm\",\r\n",
      "    ],\r\n",
      "    linkopts = [\"-ldl\", \"-lrt\"],\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "!cat /da1/s/yaolei/tensornet/examples/BUILD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLoading:\u001b[0m \n",
      "\u001b[1A\u001b[K\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
      "\u001b[1A\u001b[K\u001b[32mAnalyzing:\u001b[0m target //examples:libmodel.so (0 packages loaded, 0 targets configu\\\n",
      "red)\n",
      "\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mAnalyzed target //examples:libmodel.so (0 packages loaded, 0 targets configured).\n",
      "\n",
      "\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mFound 1 target...\n",
      "\n",
      "\u001b[1A\u001b[K\n",
      "\u001b[1A\u001b[K\u001b[32m[0 / 1]\u001b[0m [Prepa] BazelWorkspaceStatusAction stable-status.txt\n",
      "\u001b[1A\u001b[KTarget //examples:libmodel.so up-to-date:\n",
      "\u001b[32m[1 / 1]\u001b[0m checking cached actions\n",
      "\u001b[1A\u001b[K  bazel-bin/examples/libmodel.so\n",
      "\u001b[32m[1 / 1]\u001b[0m checking cached actions\n",
      "\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mElapsed time: 2.446s, Critical Path: 0.00s\n",
      "\u001b[32m[1 / 1]\u001b[0m checking cached actions\n",
      "\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0m0 processes.\n",
      "\u001b[32m[1 / 1]\u001b[0m checking cached actions\n",
      "\u001b[1A\u001b[K\u001b[32mINFO:\u001b[0m Build completed successfully, 1 total action\n",
      "\u001b[1A\u001b[K\u001b[32mINFO:\u001b[0m Build completed successfully, 1 total action\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd /da1/s/yaolei/tensornet && ./bazel build -c opt examples:libmodel.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 线上调用libmodel.so进行预测\n",
    "- 线上预测可以直接通过dlopen进行调用，`Run`函数对应graph.cc中定义的`Run`函数。具体的调用方式可以参考`/da1/s/yaolei/tensornet/examples/online_serving/main.cc`。\n",
    "\n",
    "  **注意：**下面例子没有实现embedding lookup的功能，使用随机数代替真实的embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <iostream>\r\n",
      "#include <chrono>\r\n",
      "#include <fstream>\r\n",
      "#include <string>\r\n",
      "#include <vector>\r\n",
      "#include <map>\r\n",
      "#include <boost/algorithm/string.hpp>\r\n",
      "#include <dlfcn.h>\r\n",
      "\r\n",
      "#include \"random.h\"\r\n",
      "\r\n",
      "using namespace std::chrono;\r\n",
      "\r\n",
      "typedef int (*RUN_FUNC)(const std::vector<std::vector<std::vector<float>>>&,\r\n",
      "                        const std::vector<int>&,\r\n",
      "                        std::vector<float>&);\r\n",
      "\r\n",
      "const int k_batch_size = 32;\r\n",
      "\r\n",
      "void InitWeight(int dim, std::vector<float>& weight) {\r\n",
      "    auto& reng = tensornet::local_random_engine();                                                                                         \r\n",
      "    auto distribution = std::normal_distribution<float>(0, 1 / sqrt(dim));\r\n",
      "\r\n",
      "    for (int i = 0; i < dim; ++i) {\r\n",
      "        weight.push_back(distribution(reng) * 0.001);\r\n",
      "    }   \r\n",
      "}\r\n",
      "\r\n",
      "int combine_fea(std::vector<std::vector<float>> emb_feas, std::vector<float>& merged_feas) {\r\n",
      "    if (emb_feas.size() % 2 != 0) {\r\n",
      "        std::cerr << \"combine_fea error.\" << std::endl;\r\n",
      "        return -1;\r\n",
      "    }\r\n",
      "    float wide_lr = 0.0;\r\n",
      "    std::vector<float> dnn_vec(8, 0.0);\r\n",
      "    for (size_t i = 0; i < emb_feas.size(); i++) {\r\n",
      "        if (i % 2 == 0) {\r\n",
      "            wide_lr += emb_feas[i][0];\r\n",
      "        } else if (i % 2 == 1) {\r\n",
      "            for (size_t j = 0; j < emb_feas[i].size(); ++j) {\r\n",
      "                dnn_vec[j] += emb_feas[i][j];\r\n",
      "            }\r\n",
      "        }\r\n",
      "    }\r\n",
      "\r\n",
      "    int count = emb_feas.size() / 2;\r\n",
      "    merged_feas.push_back(wide_lr / count);\r\n",
      "    for (auto& w : dnn_vec) {\r\n",
      "        merged_feas.push_back(w / count);\r\n",
      "    }\r\n",
      "    \r\n",
      "    return 0;\r\n",
      "}\r\n",
      "\r\n",
      "int emb_lookup(const std::vector<std::vector<std::vector<uint64_t> > >& inputs,\r\n",
      "               std::vector<std::vector<std::vector<float>>>& emb_inputs) {\r\n",
      "    for (size_t b = 0; b < inputs.size(); ++b) {\r\n",
      "        std::vector<std::vector<float>> emb_slots;\r\n",
      "        for (size_t s = 0; s < inputs[b].size(); ++s) {\r\n",
      "            int input_slot = s;\r\n",
      "            std::vector<std::vector<float>> emb_feas;\r\n",
      "            for (size_t f = 0; f < inputs[b][input_slot].size(); ++f) {\r\n",
      "                std::vector<float> weight;\r\n",
      "                // TODO : seek embedding\r\n",
      "                InitWeight(9, weight);\r\n",
      "                emb_feas.emplace_back(std::move(weight));\r\n",
      "            }\r\n",
      "            std::vector<float> merged_feas;\r\n",
      "            combine_fea(emb_feas, merged_feas);\r\n",
      "            emb_slots.emplace_back(std::move(merged_feas));\r\n",
      "        }\r\n",
      "        emb_inputs.emplace_back(std::move(emb_slots));\r\n",
      "    }\r\n",
      "\r\n",
      "    return 0;\r\n",
      "}\r\n",
      "\r\n",
      "int main() {\r\n",
      "    std::string train_slot = \"./data/slot.data\";\r\n",
      "    std::ifstream slot_if(train_slot);\r\n",
      "    if (!slot_if.is_open()) {\r\n",
      "        return -1;\r\n",
      "    }\r\n",
      "    std::string input;\r\n",
      "    getline(slot_if, input);\r\n",
      "    std::vector<std::string> slots_vec;\r\n",
      "    boost::split(slots_vec, input, boost::is_any_of(\",\"));\r\n",
      "    std::map<int, int> slot2pos;\r\n",
      "    for (int i = 0; i < slots_vec.size(); ++i) {\r\n",
      "        slot2pos[std::stoi(slots_vec[i])] = i;\r\n",
      "    }\r\n",
      "\r\n",
      "    void* handle = dlopen(\"./data/libmodel.so\", RTLD_LAZY);\r\n",
      "    if (handle == NULL) {\r\n",
      "        std::cerr << \"dlopen error.\" << std::endl;\r\n",
      "        return -1;\r\n",
      "    }\r\n",
      "    RUN_FUNC run_func = reinterpret_cast<RUN_FUNC> (dlsym(handle, \"Run\"));\r\n",
      "    if (run_func == NULL) {\r\n",
      "        std::cerr << \"get Run error.\" << std::endl;\r\n",
      "        return -1;\r\n",
      "    }\r\n",
      "\r\n",
      "\r\n",
      "    std::string file_name = \"./data/feature.data\";\r\n",
      "    std::ifstream data_if(file_name);\r\n",
      "    if (!data_if.is_open()) {\r\n",
      "        return -1;\r\n",
      "    }\r\n",
      "\r\n",
      "    std::vector<std::vector<std::vector<uint64_t> > > inputs;\r\n",
      "    std::vector<std::vector<std::vector<float>>> emb_inputs;\r\n",
      "    std::vector<int> dim = {1, 8};\r\n",
      "    while(getline(data_if, input)) {\r\n",
      "        std::vector<std::vector<uint64_t> > one_input;\r\n",
      "        one_input.assign(slot2pos.size(), {});\r\n",
      "        std::vector<std::string> vec;\r\n",
      "        boost::split(vec, input, boost::is_any_of(\"\\t\"));\r\n",
      "        for (size_t i = 1; i < vec.size(); ++i) {\r\n",
      "            std::vector<uint64_t> feas;\r\n",
      "            std::vector<std::string> slot_feas;\r\n",
      "            boost::split(slot_feas, vec[i], boost::is_any_of(\"\\001\"));\r\n",
      "            int index;\r\n",
      "            if (slot2pos.count(std::stoi(slot_feas[0])) > 0) {\r\n",
      "                index = slot2pos[std::stoi(slot_feas[0])];\r\n",
      "                std::vector<std::string> feas_vec;\r\n",
      "                boost::split(feas_vec, slot_feas[1], boost::is_any_of(\"\\002\"));\r\n",
      "                for (auto& fea : feas_vec) {\r\n",
      "                    feas.push_back(std::stoull(fea));\r\n",
      "                }\r\n",
      "            } else {\r\n",
      "                continue;\r\n",
      "            }\r\n",
      "            one_input[index] = feas;\r\n",
      "        }\r\n",
      "        inputs.emplace_back(one_input);\r\n",
      "\r\n",
      "        if (inputs.size() % k_batch_size == 0) {\r\n",
      "            emb_lookup(inputs, emb_inputs);\r\n",
      "            std::vector<float> outputs;\r\n",
      "            auto start = system_clock::now();\r\n",
      "            run_func(emb_inputs, dim, outputs);\r\n",
      "            auto end   = system_clock::now();\r\n",
      "            auto duration = duration_cast<microseconds>(end - start);\r\n",
      "\r\n",
      "            std::cout << \"cost:\" << double(duration.count()) << std::endl;\r\n",
      "            for (int i = 0; i < outputs.size(); ++i) {\r\n",
      "                std::cout << outputs[i] << std::endl;\r\n",
      "            }\r\n",
      "            inputs.clear();\r\n",
      "            outputs.clear();\r\n",
      "            emb_inputs.clear();\r\n",
      "        }\r\n",
      "    }\r\n",
      "\r\n",
      "    if (inputs.size() != 0) {\r\n",
      "        emb_lookup(inputs, emb_inputs);\r\n",
      "        std::vector<float> outputs;\r\n",
      "        auto start = system_clock::now();\r\n",
      "        run_func(emb_inputs, dim, outputs);\r\n",
      "        auto end   = system_clock::now();\r\n",
      "        auto duration = duration_cast<microseconds>(end - start);\r\n",
      "\r\n",
      "        std::cout << \"cost:\" << double(duration.count()) << std::endl;\r\n",
      "        for (int i = 0; i < outputs.size(); ++i) {\r\n",
      "            std::cout << outputs[i] << std::endl;\r\n",
      "        }\r\n",
      "    }\r\n",
      "\r\n",
      "\r\n",
      "    return 0;\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat /da1/s/yaolei/tensornet/examples/online_serving/main.cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLoading:\u001b[0m \r\n",
      "\r",
      "\u001b[1A\u001b[K\u001b[32mLoading:\u001b[0m 0 packages loaded\r\n",
      "\r",
      "\u001b[1A\u001b[K\u001b[32mAnalyzing:\u001b[0m target //examples:tf_serving (0 packages loaded, 0 targets configur\\\r\n",
      "ed)\r\n",
      "\r",
      "\u001b[1A\u001b[K\r",
      "\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mAnalyzed target //examples:tf_serving (0 packages loaded, 0 targets configured).\r\n",
      "\r\n",
      "\r",
      "\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mFound 1 target...\r\n",
      "\r\n",
      "\r",
      "\u001b[1A\u001b[K\u001b[32m[0 / 1]\u001b[0m [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\n",
      "\r",
      "\u001b[1A\u001b[KTarget //examples:tf_serving up-to-date:\r\n",
      "\u001b[32m[1 / 1]\u001b[0m checking cached actions\r\n",
      "\r",
      "\u001b[1A\u001b[K  bazel-bin/examples/tf_serving\r\n",
      "\u001b[32m[1 / 1]\u001b[0m checking cached actions\r\n",
      "\r",
      "\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mElapsed time: 0.141s, Critical Path: 0.00s\r\n",
      "\u001b[32m[1 / 1]\u001b[0m checking cached actions\r\n",
      "\r",
      "\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0m0 processes.\r\n",
      "\u001b[32m[1 / 1]\u001b[0m checking cached actions\r\n",
      "\r",
      "\u001b[1A\u001b[K\u001b[32mINFO:\u001b[0m Build completed successfully, 1 total action\r\n",
      "\r",
      "\u001b[1A\u001b[K\u001b[32mINFO:\u001b[0m Build completed successfully, 1 total action\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd /da1/s/yaolei/tensornet && ./bazel build -c opt examples:tf_serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 准备运行环境，将编译的libmodel.so和tf_serving放到下面测试目录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/da1/s/yaolei/tensornet/examples/online_serving/test_env/\r\n",
      "├── data\r\n",
      "│   ├── feature.data\r\n",
      "│   ├── libmodel.so\r\n",
      "│   └── slot.data\r\n",
      "└── tf_serving\r\n",
      "\r\n",
      "1 directory, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree /da1/s/yaolei/tensornet/examples/online_serving/test_env/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  slot.data中slot顺序需要和wide_deep.py中的WIDE_SLOTS和DEEP_SLOTS顺序一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3,4\r\n"
     ]
    }
   ],
   "source": [
    "!cat /da1/s/yaolei/tensornet/examples/online_serving/test_env/data/slot.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  feature.data数据按照main.cc中解析的格式构造即可，下面有些特殊分隔符显示不对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\u0001-1956697246319764053\t2\u0001-5730244542641024933\t3\u0001-9118175470622903910\u0002-8448113875518360108\t4\u0001-2457261431940944054\r\n",
      "0\t1\u0001-1956697246319764053\t2\u0001-1160342140770244045\t3\u0001-928246004650238241\u00026746402536336743089\t4\u00018069461642963552018\r\n",
      "0\t1\u0001-1956697246319764053\t2\u0001-7395780378584928338\t3\u0001-8198260618690915435\u00025518680928552928316\t4\u00011517509480232003656\r\n",
      "0\t1\u0001-1956697246319764053\t2\u00015418959032072182947\t3\u0001-7328057248110740505\u00022638487947984888231\t4\u0001-3889211362256458670\r\n",
      "0\t1\u0001-1956697246319764053\t2\u0001-1447814788092430700\t3\u00011314136415692403795\u00025677372407420628171\t4\u00018002349267150817951\r\n",
      "0\t1\u0001-1956697246319764053\t2\u0001-3444864941673928379\t3\u00014367684639007080916\u00028517447599233938262\t4\u00017249151252390487352\r\n",
      "0\t1\u0001-1956697246319764053\t2\u0001-25308817390645703\t3\u0001-7048581416920648308\u0002415903991668524816\t4\u00011517509480232003656\r\n",
      "0\t1\u0001-1956697246319764053\t2\u0001-3444864941673928379\t3\u00015380535544434298816\u00024526355427571578606\t4\u0001-4517525648429849306\r\n",
      "0\t1\u0001-1956697246319764053\t2\u0001-3444864941673928379\t3\u00015380535544434298816\u00025299058237759487470\t4\u0001-4614089404967711381\r\n"
     ]
    }
   ],
   "source": [
    "!cat /da1/s/yaolei/tensornet/examples/online_serving/test_env/data/feature.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  执行`tf_serving`，输出预测结果。由于此demo没有使用真实的embedding，所以预测结果不可信。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517147\r\n",
      "0.517147\r\n",
      "0.517104\r\n",
      "0.517199\r\n",
      "0.517208\r\n",
      "0.517205\r\n",
      "0.517218\r\n",
      "0.517072\r\n",
      "0.517067\r\n"
     ]
    }
   ],
   "source": [
    "!cd /da1/s/yaolei/tensornet/examples/online_serving/test_env/ && ./tf_serving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
